{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Fundamentals, Quick Tutorial\n",
    "\n",
    "Esto es solo una transcripción del siguiente tutorial para tenerlo todo en un solo lugar.\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html\n",
    "\n",
    "These are multidimensional arrays that can be stored in GPU.\n",
    "\n",
    "Los tensores además tienen a heck of a lot of métodos que se pueden usar para diversas cosas. \n",
    "Muchos de estos métodos y parámetros de los tensores se usan en autograd para hacer backprop de forma automática.\n",
    "\n",
    "Mirar https://pytorch.org/docs/stable/tensors.html#tensor-class-reference para una referencia completa de las propiedades de los tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "torch.int64\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = torch.tensor([[1,2], [3,4]])\n",
    "print(x)\n",
    "x_np = np.array([[1,2], [3,4]])\n",
    "print(torch.from_numpy(x_np))\n",
    "print(x.dtype)\n",
    "print(x_np.dtype)\n",
    "\n",
    "# Por defecto los arreglos de enteros de numpy son de 32 bits, en Pytorch son de 64 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0356, 0.1533],\n",
      "        [0.2645, 0.1802]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand((2,2)))\n",
    "print(torch.ones((2,2)))\n",
    "print(torch.zeros((2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atributos de tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4]) torch.Size([3, 4]) ((3, 4)) ((3, 4)) ((3, 4))\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\",tensor.size() ,f\"({tensor.shape[0], tensor.shape[1]})\",\n",
    "f\"({tensor.size()[0], tensor.size()[1]})\", f\"({tensor.size(0), tensor.size(1)})\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operaciones con tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "  print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9129, 0.5463, 0.2383, 0.6018],\n",
      "        [0.4221, 0.3849, 0.2910, 0.6693],\n",
      "        [0.5347, 0.1561, 0.2432, 0.4349]], device='cuda:0')\n",
      "tensor([[0.0768, 0.5267, 0.2505, 0.2638],\n",
      "        [0.2097, 0.7665, 0.7592, 0.1751],\n",
      "        [0.0771, 0.6114, 0.5066, 0.4600]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(cuda_tensor)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(cpu_tensor)\n\u001b[1;32m---> 11\u001b[0m cuda_tensor \u001b[39m+\u001b[39;49m cpu_tensor\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# NO SE PUEDE OPERAR SOBRE TENSORES EN DISTINTOS DISPOSITIVOS\n",
    "\n",
    "cuda_tensor = torch.rand((3, 4), device='cuda')\n",
    "cpu_tensor = torch.rand((3, 4))\n",
    "\n",
    "print(cuda_tensor)\n",
    "print(cpu_tensor)\n",
    "\n",
    "\n",
    "\n",
    "cuda_tensor + cpu_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1.]])\n",
      "tensor([[[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "t1 = torch.cat([tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "t2 = torch.stack([tensor, tensor], dim=0)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-place operations Operations that have a _ suffix are in-place. For example: x.copy_(y), x.t_(), will change x.\n",
    "\n",
    "Operaciones con sufijo _ serán inplace y cambiarán el tensor original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuidado. Si creas un tensor desde un np array o un np.array desde un tensor su ubicación en memoria será la misma (Siempre y cuando el tensor no esté en GPU), por lo que cambiar uno afectará el otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.], device='cuda:0', dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n).to('cuda') # Esto cambiará la posición en memoria del tensor ya que este estará en la memoria de la GPU\n",
    "\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch.Autograd\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "\n",
    "Acá se hace el *heavy lifting* en el paso de backprop. Esta librería permite calcular gradientes de las funciones de activación y costo. Además, en general es una librería eficiente para cálculo vectorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at a single training step. For this example, we load a pretrained resnet18 model from torchvision. We create a random data tensor to represent a single image with 3 channels, and height & width of 64, and its corresponding label initialized to some random values. Label in pretrained models has shape (1,1000).\n",
    "\n",
    "*This tutorial work only on CPU and will not work on GPU (even if tensor are moved to CUDA)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run the input data through the model through each of its layers to make a prediction. This is the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) # forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.2230e-01, -4.3726e-01, -5.9561e-01, -1.6869e+00, -1.0063e+00,\n",
      "          4.0592e-02, -4.4441e-01,  6.9468e-01,  4.4974e-01, -6.5165e-01,\n",
      "         -1.0699e+00, -8.2337e-01, -3.2915e-01, -1.1731e+00, -1.0379e+00,\n",
      "         -7.7191e-01, -7.1098e-01, -4.0526e-01, -3.8269e-01, -4.6440e-01,\n",
      "         -1.4476e+00, -1.0471e+00, -1.5277e+00,  2.3398e-02, -8.1703e-01,\n",
      "         -8.6946e-01, -7.1315e-01, -1.0303e+00, -6.6329e-01, -1.5779e-01,\n",
      "         -8.8289e-01, -6.3415e-01, -5.2718e-01, -3.0054e-01, -4.1187e-01,\n",
      "         -2.6593e-01,  9.4039e-01, -6.0331e-01, -5.4432e-01,  2.7224e-01,\n",
      "         -5.0672e-01, -9.2970e-01, -8.1918e-01, -2.0005e-01, -6.5748e-01,\n",
      "         -2.4849e-01, -4.6830e-01, -4.8448e-01, -1.2872e+00, -1.1275e+00,\n",
      "         -1.5276e-01,  5.0956e-01, -1.5372e-01, -4.5040e-01, -1.3581e-01,\n",
      "         -9.4228e-01, -3.4223e-01, -1.5179e+00, -4.8728e-01, -2.3587e-01,\n",
      "          8.3719e-01,  7.0344e-02, -3.7713e-01, -2.2337e-02, -3.9677e-01,\n",
      "         -5.4714e-02, -4.8682e-01, -2.0644e-01, -9.3663e-01, -1.1553e+00,\n",
      "         -1.5371e+00,  8.3614e-02, -1.0234e+00, -1.1572e-01, -8.2243e-01,\n",
      "         -9.8497e-01,  4.4827e-01, -4.9830e-01,  1.4091e-01,  2.8234e-01,\n",
      "         -8.0852e-01, -1.5305e+00, -1.4122e-01, -8.1022e-01, -7.3445e-01,\n",
      "          1.9519e-01,  1.7201e-01,  3.8603e-01,  2.3042e-01, -6.8526e-01,\n",
      "         -1.1214e+00, -1.0381e+00, -2.0830e+00, -3.7302e-01,  2.3430e-01,\n",
      "         -2.0674e+00, -6.2215e-01, -3.7505e-01, -1.6682e+00, -4.7145e-01,\n",
      "         -9.7249e-01, -1.0228e+00, -1.3624e+00, -6.7255e-01, -5.0998e-01,\n",
      "         -6.0944e-01, -6.1813e-01, -1.1337e+00, -1.0814e+00, -1.3685e+00,\n",
      "         -1.2117e+00, -7.4953e-01,  8.3011e-01,  2.7829e-01,  1.4446e-01,\n",
      "         -9.3506e-01, -7.2228e-01, -5.3166e-01,  6.7740e-01, -2.1595e-01,\n",
      "         -7.4965e-01,  2.6271e-01,  2.5112e-01,  2.1443e-01,  1.0177e+00,\n",
      "         -4.8528e-02,  6.3973e-01, -1.5896e+00, -1.2030e+00, -1.2801e+00,\n",
      "         -1.5824e+00, -1.3135e+00, -1.2067e+00, -1.4829e+00, -3.3541e-01,\n",
      "         -1.4762e+00, -9.4824e-01, -8.6731e-01, -1.4085e+00, -1.6603e+00,\n",
      "         -1.6280e+00, -2.0056e+00, -2.2208e+00, -1.7277e+00, -8.9811e-01,\n",
      "         -5.4528e-01, -1.3295e+00, -1.7640e+00, -1.3091e+00, -1.1740e+00,\n",
      "         -6.5764e-03,  1.4595e+00, -9.6029e-01, -5.2974e-01, -2.5519e-01,\n",
      "          7.7631e-02, -7.0339e-02, -1.9141e-01,  2.6106e-01, -3.1462e-02,\n",
      "          5.3963e-01,  7.5369e-01,  2.6019e-01,  4.7916e-01,  5.3484e-01,\n",
      "         -9.7551e-02, -3.8119e-02, -5.0872e-01,  3.6083e-01, -1.3247e-01,\n",
      "          2.5387e-01,  8.8855e-01,  5.9522e-01,  1.5419e-01,  1.5403e-01,\n",
      "         -6.1418e-01,  2.4586e-01,  1.5104e-01,  6.3227e-01,  5.7498e-01,\n",
      "          3.7731e-01,  1.4983e-01,  6.7336e-01,  3.9478e-01,  9.5510e-01,\n",
      "          6.9888e-01,  5.2825e-01, -3.3554e-02,  3.1177e-01,  7.7734e-01,\n",
      "         -4.2130e-01,  4.5431e-01,  2.6091e-01,  5.2673e-01, -5.8532e-01,\n",
      "          9.4058e-01,  1.8854e-01,  3.4291e-01,  4.2149e-01,  7.1790e-01,\n",
      "          1.0208e-01,  2.8602e-01,  8.1532e-01,  7.9533e-01,  3.1344e-01,\n",
      "          5.0334e-01,  2.2363e-01,  5.3013e-01,  1.1551e+00,  4.5247e-01,\n",
      "         -6.1372e-02, -2.8309e-02,  8.3615e-01,  3.2405e-01,  2.4718e-01,\n",
      "          4.2918e-01,  2.5007e-01,  6.3996e-01, -2.2931e-01,  7.8943e-01,\n",
      "          3.1319e-01,  2.0211e-01,  2.2764e-01,  5.4772e-01,  3.9111e-01,\n",
      "          1.6744e-01,  3.8751e-01,  6.7937e-01, -1.5402e-01, -1.0875e-02,\n",
      "          2.2120e-01,  4.0359e-01,  6.1241e-01,  7.1584e-03,  6.2326e-01,\n",
      "          6.5625e-01,  4.2874e-01,  2.2193e-01,  8.1216e-01,  1.0788e-01,\n",
      "          7.4918e-01,  2.7649e-01,  1.5339e-01,  1.0772e-01, -2.3162e-01,\n",
      "          5.1129e-01,  4.6949e-01,  1.8328e-01,  6.6604e-01,  2.1935e-01,\n",
      "          4.2870e-01,  8.5817e-01, -8.9437e-01,  5.5442e-01,  9.5101e-01,\n",
      "         -4.3694e-01,  5.0851e-01,  3.8239e-01, -1.9505e-01, -1.3450e-01,\n",
      "         -5.2181e-01, -6.7579e-01, -4.7062e-01,  3.8410e-01,  8.1497e-01,\n",
      "          8.2884e-01,  4.5608e-01,  9.3715e-01,  1.6025e-01, -4.1581e-01,\n",
      "         -8.4695e-01, -1.2497e+00, -5.3141e-01,  4.1568e-01, -1.2313e+00,\n",
      "         -9.5818e-01, -8.9214e-01, -8.2626e-01, -1.5972e+00, -6.4668e-01,\n",
      "         -4.7756e-01,  4.4213e-01,  6.7308e-01, -2.8679e-01, -9.1723e-02,\n",
      "          5.8937e-01, -2.7721e-01, -3.3969e-01, -5.0196e-01, -1.4065e+00,\n",
      "         -8.0145e-01, -1.3837e+00, -4.5281e-01, -6.4684e-01, -1.1901e+00,\n",
      "         -7.9391e-01, -9.1362e-01, -1.4178e+00, -6.4821e-01, -2.8430e-01,\n",
      "         -1.6371e+00, -3.5370e-01, -2.5564e-01, -2.2566e-01, -7.1799e-01,\n",
      "         -4.7837e-01,  6.1579e-01, -6.4683e-01, -1.0804e+00, -4.1644e-01,\n",
      "          5.6676e-01, -1.5406e-01, -2.1104e-01,  2.4183e-01,  1.0310e+00,\n",
      "         -3.2020e-01, -8.8526e-01, -8.5238e-01, -6.7675e-01, -3.6034e-01,\n",
      "         -1.5416e+00, -8.3709e-01, -1.2795e+00, -1.4579e+00, -1.2956e+00,\n",
      "         -1.4544e+00, -7.2233e-01, -6.3962e-02, -1.0546e-01, -2.3693e-01,\n",
      "         -1.2977e-01, -3.4193e-01, -1.2735e-01,  6.1359e-01, -6.0347e-01,\n",
      "         -1.0957e+00, -1.7644e+00,  2.2417e-01,  6.6365e-01, -1.2975e+00,\n",
      "         -5.4012e-01,  6.8654e-01, -3.7312e-01, -1.0256e+00, -7.9787e-01,\n",
      "          4.3740e-01, -5.1066e-01, -1.4547e+00,  1.0139e-01, -9.3331e-01,\n",
      "         -1.1383e+00, -2.4074e+00, -1.3481e+00, -8.5063e-01, -6.2479e-01,\n",
      "          6.3231e-01,  8.5473e-01, -2.1934e-01,  2.5852e-01,  1.2293e-01,\n",
      "         -3.8953e-01,  3.2555e-01, -7.2513e-02,  3.5373e-01, -4.2849e-01,\n",
      "         -5.6929e-01, -1.1450e+00, -2.4422e-01, -9.5373e-01, -4.8477e-01,\n",
      "         -7.9726e-01, -4.0836e-01, -4.8513e-01, -1.5757e-01, -1.3707e-01,\n",
      "         -5.8174e-01, -1.1621e+00,  2.6992e-01, -4.2668e-01, -6.4777e-01,\n",
      "          2.3486e-01, -3.8674e-01, -3.2545e-01, -5.2755e-01, -8.9169e-01,\n",
      "         -7.6896e-01, -1.1920e+00, -1.1432e+00, -1.0500e+00, -3.1475e-01,\n",
      "          1.0350e+00,  2.2721e-01, -1.1968e+00, -1.5406e+00,  1.6232e-01,\n",
      "          5.1219e-01, -8.7110e-01, -4.5344e-01,  3.7107e-01,  1.2880e-01,\n",
      "         -8.9379e-01,  9.2207e-01,  4.5139e-02, -2.1524e+00, -1.6334e+00,\n",
      "         -8.9002e-01, -4.9392e-01, -1.5426e-01, -1.7675e-01,  9.2026e-01,\n",
      "          1.4118e-01,  5.3065e-01,  2.1620e+00,  4.4794e-01,  5.7716e-01,\n",
      "          9.9875e-01, -4.6746e-01,  2.7123e-01,  6.5578e-02,  8.3269e-01,\n",
      "          9.1612e-01,  1.2528e+00, -1.4845e-01,  6.4907e-01,  3.9136e-01,\n",
      "         -7.5782e-01, -2.3020e-01,  1.3874e+00,  1.9019e+00,  4.8499e-01,\n",
      "         -7.1045e-01,  1.0815e-01,  3.8690e-01,  6.2840e-01,  6.6967e-01,\n",
      "          1.0333e+00, -2.7458e-01, -6.1566e-01,  2.2498e-01,  1.1866e-01,\n",
      "          1.0740e+00,  5.6936e-01,  2.1469e-02, -6.4859e-01, -4.9952e-01,\n",
      "         -1.8919e-01,  8.5157e-01,  1.6325e+00,  1.0256e+00, -6.9565e-01,\n",
      "          3.2305e-01,  2.4686e-01,  1.0068e+00, -3.3239e-01, -2.4236e-01,\n",
      "          7.5633e-01,  1.1844e+00,  1.1108e+00, -1.8820e-01,  8.3574e-01,\n",
      "         -8.4486e-01,  4.2693e-01,  1.4333e+00,  2.7074e+00,  8.6564e-01,\n",
      "         -1.9253e-01, -1.2631e+00,  1.6599e-02,  7.4141e-04,  1.8185e+00,\n",
      "          1.0218e+00,  6.4742e-01,  2.7665e-01,  7.3078e-01, -1.5595e-01,\n",
      "          2.5100e-01, -8.5233e-02,  2.2118e-01,  8.0242e-01,  3.7728e-01,\n",
      "          9.7682e-02,  3.1918e-01,  2.9293e-01, -1.0852e+00, -1.2202e+00,\n",
      "         -5.2137e-02, -3.4821e-01,  1.2100e+00,  1.4605e+00,  1.0378e+00,\n",
      "          8.8028e-02,  1.1008e+00,  1.0717e+00, -1.2353e+00,  8.0905e-01,\n",
      "         -1.0597e+00, -2.3334e-01, -6.3098e-01, -2.9419e-01,  1.2259e+00,\n",
      "         -1.5727e+00,  4.8565e-01,  1.3163e+00,  4.8809e-01,  9.3103e-01,\n",
      "          1.1576e+00,  8.8370e-01,  3.5922e-01,  2.1444e-01,  2.2660e-01,\n",
      "         -1.3479e+00, -7.6747e-01,  6.8942e-01,  3.2834e-01,  1.0412e+00,\n",
      "          1.6731e+00,  5.5621e-01, -2.3270e-01,  1.6203e+00,  8.6131e-01,\n",
      "         -6.2076e-01,  5.9267e-01,  9.5776e-01,  1.6759e+00,  3.6615e-02,\n",
      "         -7.1305e-01, -1.7970e-01, -4.3009e-01,  7.5005e-01, -4.3145e-02,\n",
      "          8.5132e-01,  1.8601e-01,  1.8267e-01, -8.6329e-01,  8.6674e-02,\n",
      "         -7.6213e-01, -5.9631e-01, -4.3065e-01,  2.1354e-01,  9.8760e-01,\n",
      "         -1.2317e+00,  1.5680e+00,  9.7036e-01,  6.0076e-01,  6.5967e-01,\n",
      "          8.3039e-01,  5.3526e-01, -2.1149e+00, -1.4312e+00,  5.6443e-02,\n",
      "         -3.9450e-01,  1.1145e-01,  9.0985e-01, -2.3016e-01, -1.6635e+00,\n",
      "         -7.5309e-01, -1.0159e-01,  1.8362e-01,  1.1732e+00,  9.4959e-01,\n",
      "          1.9942e-01, -1.1830e-01,  6.0418e-01, -1.2808e-01, -1.5002e+00,\n",
      "         -9.1214e-01, -1.5865e-02,  1.0819e+00,  6.7041e-01, -3.3228e-01,\n",
      "          1.2141e+00,  1.5499e-01,  8.2036e-01, -8.0070e-01,  7.0844e-01,\n",
      "          7.8949e-02, -7.7516e-01,  8.4052e-01,  2.6131e-01,  1.9738e-01,\n",
      "          1.2769e-01, -1.9572e-01,  6.2038e-01,  7.7412e-01,  5.8103e-01,\n",
      "          5.9201e-01, -4.2898e-01,  1.3278e+00,  9.7117e-01,  1.3716e+00,\n",
      "         -6.2038e-01,  4.3150e-01, -7.4408e-01,  7.5593e-01,  2.8607e-01,\n",
      "         -6.5494e-01,  6.7711e-01, -5.2424e-01, -6.3222e-01,  9.1969e-01,\n",
      "          1.7505e+00, -4.1202e-02, -4.3970e-01, -5.5371e-01,  1.0037e-01,\n",
      "          1.8732e-01,  1.0923e+00, -5.3110e-01,  6.0105e-01, -1.7717e-01,\n",
      "          9.3801e-01,  3.6183e-01, -5.8956e-01,  5.0454e-01, -1.6761e-01,\n",
      "          4.8825e-01,  1.2061e+00,  7.7206e-01,  1.9943e+00,  7.6098e-01,\n",
      "          1.2841e+00,  7.1084e-01,  3.0023e-01,  2.2784e-01, -1.9659e-01,\n",
      "         -1.1660e+00,  8.9435e-01, -2.4577e-01, -1.3773e+00,  2.3722e-01,\n",
      "         -3.1595e-02,  8.2450e-01,  6.8237e-01,  1.3437e+00, -3.1764e-01,\n",
      "          2.4184e-01,  1.5383e+00,  1.0369e+00,  5.2966e-01,  8.4109e-02,\n",
      "         -1.7549e+00,  1.0400e+00, -2.9400e-02,  1.2168e+00,  6.1619e-01,\n",
      "         -1.0625e+00,  3.0842e-01,  4.2499e-01, -7.7136e-01, -1.3591e+00,\n",
      "          9.2978e-01, -2.1587e-02,  5.6938e-01,  9.2990e-01, -3.2683e-02,\n",
      "          6.2646e-01,  2.4687e-01, -9.0370e-02,  7.4847e-02,  5.3322e-01,\n",
      "         -3.0419e-01, -7.6986e-01, -8.3312e-02, -1.1981e+00,  1.0205e+00,\n",
      "         -1.0304e-01,  1.1498e+00,  4.1792e-01, -9.9660e-01, -4.0876e-01,\n",
      "          2.2674e-01, -2.2340e-01, -2.6618e-01,  3.8107e-01,  1.5984e+00,\n",
      "         -6.9556e-01,  1.6133e+00,  8.4503e-01,  1.0766e+00,  6.8747e-02,\n",
      "          5.0170e-01,  6.5671e-01, -6.8688e-01,  6.5379e-01,  4.7702e-01,\n",
      "         -1.4000e+00, -2.2813e-01, -1.5809e+00, -3.8526e-01, -8.7609e-01,\n",
      "         -6.2215e-01,  6.6848e-01,  7.9593e-01,  5.7924e-01, -6.2278e-01,\n",
      "          7.1764e-01,  1.3468e+00,  9.9495e-02, -6.5469e-01,  1.2510e-01,\n",
      "          1.7860e+00, -9.1200e-02, -2.0849e-01,  7.4252e-01,  7.2367e-01,\n",
      "         -3.7472e-01, -5.8863e-01,  6.3491e-01,  6.8660e-01,  1.6426e-01,\n",
      "          6.2295e-01,  8.9230e-01,  1.7777e-01, -6.5678e-01,  4.3787e-01,\n",
      "         -7.3099e-01,  6.2075e-01, -8.4711e-01, -5.1487e-01,  4.9523e-01,\n",
      "          1.4235e-01,  4.2428e-01,  1.2954e+00,  1.8513e-01, -6.2194e-01,\n",
      "          1.2554e+00, -1.0427e+00,  6.3980e-02,  1.4314e+00, -6.4554e-01,\n",
      "         -3.1062e-01,  2.3129e+00, -5.6312e-01,  1.8374e+00, -1.4076e+00,\n",
      "          5.6130e-02, -7.8547e-02,  8.1249e-01,  6.9397e-01,  6.8221e-02,\n",
      "          1.1029e+00, -3.4602e-01,  4.2278e-01,  3.3363e-01,  4.3859e-01,\n",
      "          3.0202e-01,  2.2527e-01,  8.0321e-01,  8.0954e-01,  1.5979e+00,\n",
      "          1.9268e-01, -1.1047e-01,  5.5599e-01,  3.4983e-01,  6.5245e-01,\n",
      "         -2.1771e-01,  9.8815e-01, -2.4985e-01,  1.3528e+00, -1.0683e-02,\n",
      "          9.3623e-02,  4.1890e-01,  4.1623e-01,  2.2319e-01,  1.0525e+00,\n",
      "          7.4856e-01,  3.3487e-01,  3.0268e-01, -1.5074e-01,  1.1893e+00,\n",
      "          5.1317e-01,  5.0289e-01,  1.1772e+00,  7.1129e-01,  7.4903e-01,\n",
      "          3.6155e-01,  4.4354e-01,  1.5199e-01,  1.4074e+00, -7.2248e-01,\n",
      "         -1.1418e+00, -4.9965e-01,  9.7481e-01,  7.5425e-01,  1.2074e+00,\n",
      "          7.3830e-01,  4.8162e-01,  1.2000e+00,  3.8600e-01, -3.6854e-01,\n",
      "          8.2513e-01,  1.3013e+00,  1.4582e+00,  1.3012e+00,  1.2178e-01,\n",
      "          5.5379e-01,  1.3090e+00,  1.0211e+00, -8.0047e-01,  2.7743e-01,\n",
      "         -6.7526e-01,  1.4896e-01, -6.1522e-01, -1.1247e+00,  9.7312e-01,\n",
      "          1.2476e+00,  2.9342e-01,  3.3976e-01,  1.3832e+00,  3.1135e-01,\n",
      "         -7.1977e-01,  1.0215e+00, -1.2325e-01,  1.4637e+00, -1.1635e+00,\n",
      "         -9.6407e-02,  1.2779e-01, -1.0562e+00,  1.7206e+00,  4.1733e-01,\n",
      "         -1.3790e+00, -1.1053e+00,  6.1624e-01,  7.8858e-01,  8.4877e-01,\n",
      "         -9.0734e-01,  2.1068e-01,  6.5504e-01,  1.3752e+00, -3.3808e-01,\n",
      "          1.2421e+00,  4.0305e-01, -9.6340e-01, -9.1381e-01,  9.9647e-02,\n",
      "          3.6376e-01,  1.4788e+00,  1.6124e+00,  8.2724e-01, -6.4465e-01,\n",
      "          1.6804e+00,  7.6453e-01,  1.5319e-01,  6.0209e-01,  4.3135e-01,\n",
      "          1.6659e+00,  6.4785e-01, -6.5038e-01,  2.9389e-01,  8.8777e-01,\n",
      "          1.5304e+00,  1.3083e+00,  1.9603e+00, -7.7411e-01, -5.1095e-01,\n",
      "          5.2768e-01, -4.4542e-01, -1.6765e-01, -9.3472e-01,  1.1649e+00,\n",
      "         -5.3096e-02,  1.4267e+00,  9.4238e-01, -1.0738e-01, -6.7068e-01,\n",
      "          7.0216e-01,  1.5019e-01, -2.4687e-01,  1.7419e+00, -3.7379e-01,\n",
      "          6.3216e-01, -1.4456e+00,  1.0930e+00, -1.0967e+00, -2.3153e+00,\n",
      "          2.4460e-01,  1.4836e+00, -1.9200e-01, -2.9208e-01,  1.5085e+00,\n",
      "          1.1310e+00, -4.0088e-01,  1.4032e+00,  1.2081e+00, -3.8444e-01,\n",
      "          4.2315e-01, -1.9066e-01, -4.4536e-01, -1.2521e+00,  7.2941e-02,\n",
      "         -4.4401e-01,  5.6878e-01,  6.8889e-01,  2.0943e-01, -8.3759e-01,\n",
      "         -4.8492e-01,  1.0891e+00,  6.0926e-01,  2.0442e+00,  1.9727e+00,\n",
      "         -1.0782e+00, -6.1536e-01,  1.3961e+00,  9.1678e-01,  2.7878e-01,\n",
      "         -3.3771e-01, -7.1512e-01,  1.3563e+00, -5.5191e-01,  8.1403e-01,\n",
      "          1.1160e+00,  1.1185e+00,  7.0811e-01,  2.4379e-02, -1.9396e+00,\n",
      "         -2.8216e-01,  7.1336e-02,  4.3008e-01,  4.0960e-01,  4.7104e-01,\n",
      "          1.7942e-01,  1.3121e+00, -3.9127e-01,  5.7458e-01, -1.5422e-01,\n",
      "         -6.3064e-01, -8.3986e-01, -4.7160e-01, -2.0410e-02,  1.3389e+00,\n",
      "         -4.3508e-01, -1.6160e-01,  5.1766e-02, -1.5364e+00,  1.1913e-01,\n",
      "         -4.6761e-01,  6.4241e-01,  4.2110e-01,  2.1634e-01, -8.2704e-03,\n",
      "         -1.8568e-01, -3.8113e-01, -3.9665e-01,  3.7531e-01, -5.7935e-01,\n",
      "         -5.9371e-01, -1.3667e+00,  4.4565e-01,  7.6148e-01, -2.8446e-02,\n",
      "          6.2268e-02, -5.8954e-01, -4.6390e-01,  7.7544e-02,  5.8639e-01,\n",
      "         -2.4998e-01, -5.7335e-02, -4.8573e-01, -5.3226e-02, -9.4449e-01,\n",
      "          3.2508e-01,  2.9001e-01, -1.2635e-01, -6.0073e-01, -1.1076e+00,\n",
      "          1.0193e-01,  7.7474e-01, -6.9211e-01,  8.0368e-01, -1.5466e-01,\n",
      "          2.3882e-01,  9.7161e-01, -1.6701e-02, -3.4121e-01, -2.0474e+00,\n",
      "          9.8383e-01, -1.7153e+00,  2.6686e-01, -1.8834e-01, -7.1416e-01,\n",
      "         -8.3640e-01,  5.3390e-02,  6.2208e-01, -8.1079e-02, -6.3797e-01,\n",
      "         -1.1479e+00, -2.4284e+00,  1.4586e+00, -3.2426e-01, -8.3725e-01,\n",
      "         -5.0204e-01, -1.3307e+00, -7.3562e-01, -2.1642e+00, -5.9802e-01,\n",
      "         -6.7139e-01,  1.3436e-01, -5.6526e-01,  1.1764e+00,  1.0815e+00]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(prediction) # Note que un atributo del objeto tensor es la función de gradiente (grad_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddmmBackward0 object at 0x000001913DE06CD0>\n"
     ]
    }
   ],
   "source": [
    "print(prediction.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the model’s prediction and the corresponding label to calculate the error (loss). The next step is to backpropagate this error through the network. Backward propagation is kicked off when we call .backward() on the error tensor. Autograd then calculates and stores the gradients for each model parameter in the parameter’s .grad attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SubBackward0 object at 0x000001913DA6A7C0>\n",
      "<SumBackward0 object at 0x000001913DE06F70>\n"
     ]
    }
   ],
   "source": [
    "print((prediction - labels).grad_fn)\n",
    "print((prediction - labels).sum().grad_fn) # Note que los tensores van guardando la última función que los generó.\n",
    "\n",
    "# Si un tensor X representa una capa del modelo, al hacer loss.backward() al tensor del vector de costo, el autograd,\n",
    "#  automáticamente guardará el gradiente calculado de dicha capa en X.grad, incluyendo los gradientes previos \n",
    "# (Hace Backprop con una sola linea de código)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load an optimizer, in this case SGD with a learning rate of 0.01 and momentum of 0.9. We register all the parameters of the model in the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "# Este optimizer hace uso de los gradientes previamente calculados con loss.backward() para actualizar los pesos de los parámetros. \n",
    "# La forma específica de actualizar los pesos depende de los hiperparámetros del optimizados pero básicamente todos son alguna variación \n",
    "# De gradiente descendiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we call .step() to initiate gradient descent. The optimizer adjusts each parameter by its gradient stored in .grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiation in Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at how autograd collects gradients. We create two tensors a and b with requires_grad=True. This signals to autograd that every operation on them should be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create another tensor Q from a and b.\n",
    "\n",
    "$$Q = 3a^3 - b^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s assume a and b to be parameters of an NN, and Q to be the error. In NN training, we want gradients of the error w.r.t. parameters, i.e.\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial a} = 3a^2$$\n",
    "$$\\frac{\\partial Q}{\\partial b} = -2b$$\n",
    "\n",
    "When we call .backward() on Q, autograd calculates these gradients and stores them in the respective tensors’ .grad attribute.\n",
    "\n",
    "We need to explicitly pass a gradient argument in Q.backward() because it is a vector. gradient is a tensor of the same shape as Q, and it represents the gradient of Q w.r.t. itself, i.e\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial Q} = 1$$\n",
    "\n",
    "Equivalently, we can also aggregate Q into a scalar and call backward implicitly, like Q.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n",
      "tensor([36., 81.]) tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "# Gradients are now deposited in a.grad and b.grad\n",
    "\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SubBackward0 object at 0x000001914401ABB0>\n",
      "<PowBackward0 object at 0x00000191051AA220>\n",
      "<DotBackward0 object at 0x000001910539E520>\n"
     ]
    }
   ],
   "source": [
    "print(Q.grad_fn)\n",
    "print((a**3).grad_fn)\n",
    "print((torch.dot(a,b).grad_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph\n",
    "\n",
    "Conceptually, autograd keeps a record of data (tensors) & all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
    "\n",
    "In a forward pass, autograd does two things simultaneously:\n",
    "\n",
    "- run the requested operation to compute a resulting tensor, and\n",
    "\n",
    "- maintain the operation’s gradient function in the DAG.\n",
    "\n",
    "The backward pass kicks off when .backward() is called on the DAG root. autograd then:\n",
    "\n",
    "- computes the gradients from each .grad_fn,\n",
    "\n",
    "- accumulates them in the respective tensor’s .grad attribute, and using the chain rule, propagates all the way to the leaf tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclusion from DAG\n",
    "\n",
    "torch.autograd tracks operations on all tensors which have their requires_grad flag set to True. For tensors that don’t require gradients, setting this attribute to False excludes it from the gradient computation DAG.\n",
    "\n",
    "The output tensor of an operation will require gradients even if only a single input tensor has requires_grad=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients? : False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a NN, parameters that don’t compute gradients are usually called frozen parameters. It is useful to “freeze” part of your model if you know in advance that you won’t need the gradients of those parameters (this offers some performance benefits by reducing autograd computations).\n",
    "\n",
    "Another common usecase where exclusion from the DAG is important is for finetuning a pretrained network. (https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)\n",
    "\n",
    "In finetuning, we freeze most of the model and typically only modify the classifier layers to make predictions on new labels. Let’s walk through a small example to demonstrate this. As before, we load a pretrained resnet18 model, and freeze all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s say we want to finetune the model on a new dataset with 10 labels. \n",
    "# In resnet, the classifier is the last linear layer model.fc. \n",
    "# We can simply replace it with a new linear layer (unfrozen by default) that acts as our classifier.\n",
    "\n",
    "model.fc = nn.Linear(512, 10)\n",
    "\n",
    "# Now all parameters in the model, except the parameters of model.fc, are frozen. \n",
    "# The only parameters that compute gradients are the weights and bias of model.fc.\n",
    "\n",
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "# Notice although we register all the parameters in the optimizer, \n",
    "# the only parameters that are computing gradients (and hence updated in gradient descent) \n",
    "# are the weights and bias of the classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "\n",
    "Neural networks can be constructed using the torch.nn package.\n",
    "\n",
    "Now that you had a glimpse of autograd, nn depends on autograd to define models and differentiate them. An nn.Module contains layers, and a method forward(input) that returns the output\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or weights)\n",
    "\n",
    "- Iterate over a dataset of inputs\n",
    "\n",
    "- Process input through the network\n",
    "\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "\n",
    "- Propagate gradients back into the network’s parameters\n",
    "\n",
    "- Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate * gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "<bound method Module.parameters of Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module): # Extender un nn.Module te permite que torch acceda a métodos útiles.\n",
    "\n",
    "    # Se definen como parámetros de la clase aquellas capas necesarias para su entrenamiento.\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# You just have to define the forward function, and the backward function (where gradients are computed) \n",
    "# is automatically defined for you using autograd. You can use any of the Tensor operations in the forward function.\n",
    "# The learnable parameters of a model are returned by net.parameters()\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "print(net.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try a random 32x32 input. Note: expected input size of this net (LeNet) is 32x32. To use this net on the MNIST dataset, please resize the images from the dataset to 32x32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0895,  0.0399,  0.0213,  0.1287, -0.0391,  0.1222,  0.0357, -0.0255,\n",
      "          0.0209,  0.1036]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parameters and backprops with random gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "\n",
    "torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "\n",
    "For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.\n",
    "\n",
    "If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension\n",
    "\n",
    "Before proceeding further, let’s recap all the classes you’ve seen so far.\n",
    "\n",
    "Recap:\n",
    "- torch.Tensor - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
    "\n",
    "- nn.Module - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "\n",
    "- nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
    "\n",
    "- autograd.Function - Implements forward and backward definitions of an autograd operation. Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history.\n",
    "\n",
    "At this point, we covered:\n",
    "Defining a neural network\n",
    "\n",
    "Processing inputs and calling backward\n",
    "\n",
    "Still Left:\n",
    "Computing the loss\n",
    "\n",
    "Updating the weights of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different loss functions (https://pytorch.org/docs/nn.html#loss-functions) under the nn package . A simple loss is: nn.MSELoss which computes the mean-squared error between the output and the target.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4964, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "\n",
    "# https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch\n",
    "# https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays/26999092#26999092\n",
    "target = target.view(1, -1)  # make it the same shape as output \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when we call loss.backward(), the whole graph is differentiated w.r.t. the neural net parameters, and all Tensors in the graph that have requires_grad=True will have their .grad Tensor accumulated with the gradient.\n",
    "\n",
    "For illustration, let us follow a few steps backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x0000023B8005C940>\n",
      "<AddmmBackward0 object at 0x0000023B89A95D60>\n",
      "<AccumulateGrad object at 0x0000023B89A9A460>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Acummulation\n",
    "\n",
    "In this subsection you will see why we need to zero out the gradients before each optimizer step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6., grad_fn=<CopyBackwards>)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2], [1,2]], requires_grad=True, dtype=torch.float32)\n",
    "b = torch.tensor([[1,1], [1,1]], requires_grad=True, dtype=torch.float32)\n",
    "l = torch.norm(a @ b)\n",
    "print(l)\n",
    "l.backward()\n",
    "# This should be the gradients of the loss function with respect to a and b\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6., grad_fn=<CopyBackwards>)\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[2., 2.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# If we calculate the gradients again with respect to the same loss they should be the same right ?. Well, no by default\n",
    "l = torch.norm(a @ b)\n",
    "print(l)\n",
    "l.backward()\n",
    "# This should be the gradients of the loss function with respect to a and b\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# The new gradient will add up to the previous gradients. So we actually need to set de previous gradients to zero for the correct behaviour\n",
    "l = torch.norm(a @ b)\n",
    "a.grad.zero_(); b.grad.zero_() # The optimizer does this automagically for all the trainable parameters of the net\n",
    "l.backward()\n",
    "# Now we have the correct gradients\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop and Updating Weights\n",
    "\n",
    "To backpropagate the error all we have to do is to loss.backward(). You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n",
    "\n",
    "Now we shall call loss.backward(), and have a look at conv1’s bias gradients before and after the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0109, -0.0071, -0.0042,  0.0141, -0.0094, -0.0060])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "weight = weight - learning_rate * gradient\n",
    "\n",
    "We can implement this using simple Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.1566,  0.0341, -0.1621,  0.1380, -0.0677],\n",
      "          [-0.0213, -0.0179, -0.1068, -0.1274, -0.0610],\n",
      "          [-0.1949,  0.1491, -0.0670,  0.0372, -0.1481],\n",
      "          [ 0.0995, -0.1996, -0.0411, -0.1645,  0.1799],\n",
      "          [-0.1841, -0.1797,  0.1347, -0.1499, -0.0646]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0135,  0.0778, -0.1069,  0.1800, -0.0954],\n",
      "          [ 0.0735, -0.1567,  0.1787, -0.0599,  0.1315],\n",
      "          [-0.0134, -0.1391,  0.1753, -0.1073,  0.1062],\n",
      "          [ 0.1628, -0.0809,  0.0987, -0.0721, -0.0882],\n",
      "          [-0.1521, -0.1999, -0.0265, -0.1386,  0.0986]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1482, -0.1414, -0.0380, -0.1731, -0.1218],\n",
      "          [ 0.0227, -0.1114, -0.0913,  0.0702, -0.1882],\n",
      "          [-0.0413, -0.1908, -0.1205,  0.0140,  0.1962],\n",
      "          [-0.0059, -0.0548,  0.1488, -0.0507, -0.0149],\n",
      "          [-0.1564,  0.1860, -0.0853,  0.1160, -0.0699]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1244,  0.1516, -0.0403, -0.0849,  0.0339],\n",
      "          [-0.1446,  0.0289, -0.0119,  0.0756, -0.0778],\n",
      "          [-0.0772,  0.1723,  0.0006,  0.0499, -0.0005],\n",
      "          [ 0.0753,  0.0899, -0.0861,  0.1282,  0.1393],\n",
      "          [ 0.1117,  0.0331,  0.1079,  0.0043, -0.0306]]],\n",
      "\n",
      "\n",
      "        [[[-0.1355, -0.0341, -0.1461,  0.0070, -0.0981],\n",
      "          [ 0.0262, -0.1553, -0.0217,  0.1100, -0.0537],\n",
      "          [-0.0934, -0.0350, -0.0205, -0.0368, -0.0487],\n",
      "          [ 0.0021,  0.1648, -0.0263,  0.0358,  0.0594],\n",
      "          [-0.1237,  0.1548, -0.0028,  0.0959, -0.0290]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1806,  0.1625, -0.0735, -0.1632,  0.0302],\n",
      "          [ 0.1823,  0.1056,  0.0462,  0.1860,  0.0370],\n",
      "          [-0.1961,  0.0737,  0.0130,  0.0372,  0.0890],\n",
      "          [-0.0550,  0.0741,  0.0230,  0.0763,  0.1784],\n",
      "          [-0.1620,  0.0701,  0.0149, -0.1416,  0.1724]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(next(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'builtin_function_or_method'>\n"
     ]
    }
   ],
   "source": [
    "print(type(next(net.parameters()).data))\n",
    "print(type(next(net.parameters()).data.sub_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: torch.optim that implements all these methods. Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dl_arch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ae4df1c5943cf4748546a76d45f227f53fb8acff87095bbb53afeb5cd17aec1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
