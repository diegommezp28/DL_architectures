{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this one time if pytorch is not already installed in the current juyter kernel \n",
    "# !conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\" \n",
    "\n",
    "print(device)\n",
    "x = torch.ones(1, device=device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch framework can be installed with a copy of the CUDA toolkit by nvidia so it can have access to the current device dedicated GPU if there is one.\n",
    "The `device` variable will have the GPU availability information so we can now if our code can be run in the GPU insted of the CPU. For more infor on install options go to: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "In this notebook we will be implementing one of the most popular architectures that has taken over the DL community, Tranformers!!. \n",
    "They started with the 2017 paper by Google called \"Attention is all you need\" since the main idea behind transformers is to use the so calles \"attention mechanism\" as the core part of the architecture. This started as a sequence modeling architecture, more specifically for Language modelling, but in recent year it has taken over almost every big field of Deep Learning. Vision with ViT, RL with Decision Transformers, Speech with the Conformer and many more. One of my goals with this project is to implement all of these architectures and see how they compare with more traditional approaches like LSTM for Language, CNN for vision and Encoder-Decoder for Speech.\n",
    "\n",
    "In particular in this notebook we will be implementing the core and basic transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Attention Mechanism\n",
    "\n",
    "Note: Here all vectors are always column vectors, so $x \\in R^{n}$ is a column of size $n$ and $x^T$ a row of size $n$\n",
    "\n",
    "The key idea behind transformers, as said above, is the attention mechanism. This can be represented as 3 matrices that act as parameters of the model. Query, Key, Value are the usual names given to the said matrices. \n",
    "Self attention is a sequence to sequence operator, so it inputs *t* vectors, each in $R^{n}$ and outputs also *t* vectors in $R^{n}$ (in the context of NLP each vector is a word embedding, and the *t* vectors represent a sentence of *t* words). Another way of seeing it is that is takes an element of $R^{n x t}$ and outputs another element of $R^{n x t}$. So one way of doing this is to multiply the entry matrix of vectors (lets call it $X \\in R^{n x t}$) with a matrix $W$ of size (t x t) so the output is another matrix (lets call it $Y \\in R^{n x t}$). This is exaclty what the basic attention mechanism does. Basically each output column vector $y_i \\in R^{n}$ is calculates like:\n",
    "\n",
    "$$ y_i = \\sum_j w_{ij}x_j \\text{ where } w_{ij} = softmax(x_i^T \\cdot X, \\text{row wise})_j \\text{ .j-th entry of the vector obtain by applying a softmax opperation. } $$\n",
    "\n",
    "Sea $w_i \\in R^t := softmax(x_i^T \\cdot X, \\text{row wise})^T$ entonces: \n",
    "\n",
    "$$ y_i = X \\cdot w_i$$\n",
    "\n",
    "más aún, sea $W := softmax(X^T \\cdot X, \\text{row wise})^T = [w_1 | ... | w_i | ... | w_t]$\n",
    "\n",
    "$$ Y =  X \\cdot W = [y_1 | ... | y_i | ... | y_t]$$\n",
    "\n",
    "So We have that: \n",
    "\n",
    "$$ Y = X \\cdot softmax(X^T \\cdot X, \\text{row wise})^T$$\n",
    "\n",
    "Note that since $X^T \\cdot X$ is a symmetric matrix (This is really easy to check), the above formula can also be written as:\n",
    "\n",
    "$$ Y = X \\cdot softmax(X \\cdot X^T, \\text{row wise})^T$$\n",
    "\n",
    "But we also know that $softmax(B, \\text{row wise})^T = softmax(B^T, \\text{Column wise})$. Then:\n",
    "\n",
    "$$ Y = X \\cdot softmax(X^T \\cdot X, \\text{column wise})$$\n",
    "\n",
    "We can appreciate that in the formula above the entry matrix $X$ appear 3 times. What self attentiont does is to replicate this behaviour with 3 different matrices that will be parameters that the model needs to optimize via back propagation. \n",
    "\n",
    "So, in self-attention the role of the first appearence of X is made by the Value matrix. The second is the Query and the Third is the Key. So the formula for $Y$ becomes:\n",
    "\n",
    "$$\n",
    "Y = V \\cdot softmax(Q^T \\cdot K, \\text{column wise})\n",
    "$$\n",
    "\n",
    "Where $Q, K \\in R^{n x t}$ and $V \\in R^{txt}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_self_att_1():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "tensor([[1., 2., 3.],\n",
      "        [2., 2., 4.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [2., 2., 4.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([[0.0900, 0.1065, 0.0900],\n",
      "        [0.2447, 0.1065, 0.2447],\n",
      "        [0.6652, 0.7870, 0.6652]])\n",
      "tensor([[0.0900, 0.1065, 0.0900],\n",
      "        [0.2447, 0.1065, 0.2447],\n",
      "        [0.6652, 0.7870, 0.6652]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Softmax\n",
    "import numpy as np\n",
    "x = torch.tensor([[1,2,3], [2, 2, 4], [3, 4, 5]], dtype=torch.float32)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(torch.transpose(x, 1, 0))\n",
    "print(x.softmax(0))\n",
    "print(x.softmax(1).transpose(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Embedding\n",
    "\n",
    "The first layer of the encoder part of the transformer architecture is the input embedding, with a fixed context lenght and trainable weight the input embedding layer will have vector-based representations for each word in the input sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "     embedding_size: int = 256,\n",
    "     vocab_size: int = 20_000):\n",
    "\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.embedding_size)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potitional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PotitionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 embedding_size: int = 256,\n",
    "                 context_length: int = 256,\n",
    "                 dropout: float = 0.2):\n",
    "\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.context_length = context_length\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #create a matrix of size (context_length, embedding_size)\n",
    "        pe = torch.zeros(context_length, embedding_size)\n",
    "        # Create vector of shape (context_length)\n",
    "        position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_size, 2).float() * (-math.log(10_000.0) / embedding_size))\n",
    "        # apply sine to even positions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0) # (1, context_len, embedding_size)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, epsilon: float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon # Parameter used for numerical stability (sets an upper boundary on the scale of the normalization)\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # Multiplicative\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) # Additive\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.bias + self.alpha * (x - mean) / (std + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_size: int = 256, \n",
    "                 hidden_state_size:int = 1024, \n",
    "                 dropout: float = 0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.linear_1 = nn.Linear(embedding_size, hidden_state_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(hidden_state_size, embedding_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x -> (batch, context_size, embedding_size)\n",
    "        # -> broadcasting will perform: \n",
    "        #       linear_1 matrix of size (hidden_state_size, embedding_size) @ (batch, context_size, embedding_size) = (batch, context_size, hidden_state_size)\n",
    "        #       linear_2 matrix of size (embedding_size, hidden_state_size) @ (batch, context_size, hidden_state_size) = (batch, context_size, embedding_size)\n",
    "\n",
    "        x = self.linear_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_size: int = 256,\n",
    "                 context_length: int = 256,\n",
    "                 heads: int = 4) -> None:\n",
    "        self.embedding_size = embedding_size\n",
    "        self.context_length = context_length\n",
    "        self.heads = heads\n",
    "\n",
    "        self.tokeys    = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        self.toqueries = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        self.tovalues  = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        self.unifyheads = nn.Linear(embedding_size, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # X shape: (b, context_lenght, embd_size) -- row-wise embeddings\n",
    "        b, cl, es = x.size() # batch, context length, embedding size\n",
    "        s = es // self.heads\n",
    "\n",
    "        Q = self.toqueries(x) # --> (b, context_lenght, embd_size)\n",
    "        K = self.tokeys(x)\n",
    "        V = self.tovalues(x)\n",
    "\n",
    "\n",
    "        # Folds each head as a new batch\n",
    "        Q = Q.transpose(1, 2).contiguous().view(b * self.heads, cl, s)\n",
    "        K = K.transpose(1, 2).contiguous().view(b * self.heads, cl, s)\n",
    "        V = V.transpose(1, 2).contiguous().view(b * self.heads, cl, s)\n",
    "\n",
    "        # Scaled Dot product attention between heads\n",
    "        scaled_dot = torch.bmm(Q, K.transpose(1, 2)) / torch.sqrt(es)\n",
    "        QK_attention = torch.softmax(scaled_dot, dim=2)\n",
    "\n",
    "        # Apply learned attention\n",
    "        self_attention = torch.bmm(QK_attention, V).view(b, self.heads, cl, s)\n",
    "\n",
    "        # Concatenate heads\n",
    "        self_attention = self_attention.transpose(1, 2).contiguous().view(b, cl, s * self.heads)\n",
    "\n",
    "        return self.unifyheads(self_attention)        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size: int = 256,\n",
    "                 context_length: int = 256,\n",
    "                 heads: int = 4) -> None:\n",
    "        self.embedding_size = embedding_size\n",
    "        self.context_length = context_length\n",
    "        self.heads = heads\n",
    "\n",
    "        self.multihead_attention = MultiHeadAttention(embedding_size, context_length, heads)\n",
    "        self.layer_norm_1 = LayerNormalization()\n",
    "        self.layer_norm_2 = LayerNormalization()\n",
    "        self.mlp = FeedForward(embedding_size, hidden_state_size = 1024, dropout = 0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual_conn = x.copy()\n",
    "        x = self.multihead_attention(x)\n",
    "        x = x + residual_conn\n",
    "        x =  self.layer_norm_1(x)\n",
    "        residual_conn = x.copy()\n",
    "        x = self.mlp(x)\n",
    "        x =  self.layer_norm_2(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dl_arch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ae4df1c5943cf4748546a76d45f227f53fb8acff87095bbb53afeb5cd17aec1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
