{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Preprocessing Operations\n",
    "\n",
    "In this notebook we will implement some basic text preprocessing pipeline for NLP related tasks. \n",
    "At the end we will have a custome function that handles all the preprocessing for a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of preprocessing steps:\n",
    "* Text Normalization: \n",
    "  * Removing urls\n",
    "  * Remove Round Brackets\n",
    "  * Remove slash\n",
    "  * Remove punctuation\n",
    "  * Remove whitespace\n",
    "  * Lower case text\n",
    "  * Numbers to text\n",
    "  * Replacing contractions\n",
    "  * Removing Stop Words (Optional)\n",
    "  * Lemmatization (Optional)\n",
    "  * Stemming (Optional)\n",
    "* Tokenization:\n",
    "  * Get unique tokens (Vocab) out of the corpus. \n",
    "  * Char Ngrams vs Word Ngrams\n",
    "* Document Embedding:\n",
    "  * Types:\n",
    "    * Bag of Words or Ngrams (Bow): For each document it looks how many counts of each token it has\n",
    "    * One Hot Encoding: Fixed sice document (With Padding to max doc lenght). Each entry is the id of the token in that position. Usually word based tokens and vocabulary. \n",
    "    * TfIdf:\n",
    "\n",
    "Lemmatization and Stemming depends on the NLP task to make. They are most usefull for text clasiffication for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to use some benchmark data\n",
    "# !pip install torchdata \n",
    "# !pip install torchtext\n",
    "\n",
    "# Run this just once if the packages are not already install in the current jupyter kernel\n",
    "# !pip install nltk\n",
    "# !pip install contractions\n",
    "# !pip install inflect\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install gensim\n",
    "\n",
    "# You might need to restart the kernel for the packages changes to take effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.datasets import AG_NEWS\n",
    "train_iter = iter(AG_NEWS(split='train'))\n",
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(AG_NEWS(split='train')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market. Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums. Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday. Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections. Stocks \""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ''\n",
    "\n",
    "for i, (tag, text) in enumerate(train_iter):\n",
    "    words += ' ' + text\n",
    "    # if i == 1000:\n",
    "    #     break\n",
    "words[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code base of: https://analyticsindiamag.com/complete-tutorial-on-text-preprocessing-in-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string, unicodedata\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing web associated noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove HTML tag\n",
    "def html_remover(data):\n",
    "  beauti = BeautifulSoup(data,'html.parser')\n",
    "  return beauti.get_text()\n",
    "\n",
    "# to remove URL\n",
    "def url_remover(data):\n",
    "  # text = re.sub(r'HREF=\"http.*\"', '', data)\n",
    "  return re.sub(r'https\\S+','', data)\n",
    "\n",
    "def web_associated(data):\n",
    "  text = html_remover(data)\n",
    "  text = url_remover(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove other noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_round_brackets(data):\n",
    "  return re.sub('\\(?\\)?','',data)\n",
    "\n",
    "def remove_slashes(data):\n",
    "  return re.sub('\\\\\\\\',' ',data)\n",
    "\n",
    "def remove_hyphens(data):\n",
    "  return re.sub('-',' ',data)\n",
    "\n",
    "def remove_punc(data):\n",
    "  trans = str.maketrans('','', string.punctuation)\n",
    "  return data.translate(trans)\n",
    "\n",
    "def white_space(data):\n",
    "  return ' '.join(data.split())\n",
    "\n",
    "def complete_noise(data):\n",
    "  new_data = web_associated(data)\n",
    "  new_data = remove_round_brackets(new_data)\n",
    "  new_data = remove_slashes(new_data)\n",
    "  new_data = remove_hyphens(new_data)\n",
    "  new_data = remove_punc(new_data)\n",
    "  new_data = white_space(new_data)\n",
    "  return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lower(data):\n",
    "  return data.lower()\n",
    "\n",
    "def contraction_replace(data):\n",
    "  return contractions.fix(data)\n",
    "\n",
    "def number_to_text(data):\n",
    "  temp_str = data.split()\n",
    "  string = ''\n",
    "  for i in temp_str:\n",
    "    # if the word is digit, converted to \n",
    "    # word else the sequence continues\n",
    "    if i.isdigit():\n",
    "      temp = inflect.engine().number_to_words(i)\n",
    "      string += ' ' + temp\n",
    "    else:\n",
    "      string += ' ' + i\n",
    "  return string\n",
    "\n",
    "def normalization(data, remove_stop_words=False):\n",
    "  text = complete_noise(data)\n",
    "  text = text_lower(text)\n",
    "  text = number_to_text(text)\n",
    "  text = contraction_replace(text)\n",
    "  if remove_stop_words:\n",
    "    text = remove_stop_words(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sysco Profit Rises; Sales Volume Flat  CHICAGO (Reuters) - Sysco Corp. &lt;A HREF=\"http://www.investor.reuters.com/FullQuote.aspx?ticker=SYY.N target=/stocks/quickinfo/fullquote\"&gt;SYY.N&lt;/A&gt;, the largest U.S.  distributor of food to restaurants and hospitals, on Monday  said quarterly profit rose as an extra week in the period and  cost control measures helped offset the higher food prices that  were slowing demand.\n",
      "\n",
      " sysco profit rises sales volume flat chicago reuters sysco corp a syyna the largest us distributor of food to restaurants and hospitals on monday said quarterly profit rose as an extra week in the period and cost control measures helped offset the higher food prices that were slowing demand\n"
     ]
    }
   ],
   "source": [
    "paragraph_raw = next(train_iter)[1]\n",
    "print(paragraph_raw)\n",
    "processed_paragraph = normalization(paragraph_raw)\n",
    "print('')\n",
    "print(processed_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' carlyle looks toward commercial aerospace reuters reuters private investment firm carlyle group which has a reputation for making well timed and occasionally controversial plays in the defense industry has quietly placed its bets on another part of the market oil and economy cloud stocks outlook reuters reuters soaring crude prices plus worries about the economy and the outlook for earnings are expected to hang over the stock market next week during the depth of the summer doldrums iraq halts oil exports from main southern pipeline reuters reuters authorities have halted oil export flows from the main pipeline in southern iraq after intelligence showed a rebel militia could strike infrastructure an oil official said on saturday oil prices soar to all time record posing new menace to us economy afp afp tearaway world oil prices toppling records and straining wallets present a new economic menace barely three months before the us presidential elections stocks end up but near year lows r'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_words = normalization(words)\n",
    "norm_words[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28497158\n",
      "28503360\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(len(norm_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Basic tokenization.\n",
    "We could also use some more specific libraries such as SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carlyle',\n",
       " 'looks',\n",
       " 'toward',\n",
       " 'commercial',\n",
       " 'aerospace',\n",
       " 'reuters',\n",
       " 'reuters',\n",
       " 'private',\n",
       " 'investment',\n",
       " 'firm']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(norm_words)\n",
    "tokens[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4681652"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carlyl',\n",
       " 'look',\n",
       " 'toward',\n",
       " 'commerci',\n",
       " 'aerospac',\n",
       " 'reuter',\n",
       " 'reuter',\n",
       " 'privat',\n",
       " 'invest',\n",
       " 'firm']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stemmed_words = [stemmer.stem(w) for w in tokens]\n",
    "stemmed_words[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Vocabulary from tokens. \n",
    "In this context a vocanulary is just a dict that assigns a unique index to each token and has the counts for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# This expects list of list of tokens, if i just pass list of tokens it considers the letters as the tokens\n",
    "vocab = build_vocab_from_iterator([tokens], max_tokens=2000) #Take into account just the 2000 more common words\n",
    "vocab.set_default_index(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2000, 1066, 856, 1272, 2000, 25, 25, 898, 785, 335]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_indices(tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'to', 'a', 'of', 'in']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing, tokenizing and Building Vocab Using Gensim\n",
    "\n",
    "También podemos hacer lo anterior de forma un poco menos manual usando la librería Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carlyl',\n",
       " 'look',\n",
       " 'commerci',\n",
       " 'aerospac',\n",
       " 'reuter',\n",
       " 'reuter',\n",
       " 'privat',\n",
       " 'invest',\n",
       " 'firm',\n",
       " 'carlyl']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "gprep_words = preprocess_string(words) #Básicamente preprocesa y te devuleve tokens de una\n",
    "gprep_words[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que el preprocesamiento por defecto ya lo hace bastante bien. Aunque de igual forma podemos agregar filtros personalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carlyl',\n",
       " 'look',\n",
       " 'commerci',\n",
       " 'aerospac',\n",
       " 'reuter',\n",
       " 'reuter',\n",
       " 'privat',\n",
       " 'invest',\n",
       " 'firm',\n",
       " 'carlyl']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import strip_tags, remove_stopwords, strip_short, stem_text\n",
    "\n",
    "CUSTOM_FILTERS = [normalization, strip_tags, remove_stopwords, strip_short, stem_text]\n",
    "words_custom_filter = preprocess_string(words, filters=CUSTOM_FILTERS)\n",
    "words_custom_filter[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2000, 135, 1011, 2000, 3, 3, 636, 433, 177, 2000]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_custom_vocab = build_vocab_from_iterator([words_custom_filter], max_tokens=2000) #Take into account just the 2000 more common words\n",
    "g_custom_vocab.set_default_index(2000)\n",
    "g_custom_vocab.lookup_indices(words_custom_filter[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En conclusión. El pipeline por defecto de gensim es bastante bueno. Solo le agregaría filtros para lidiar con url y para convertir números a palabras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dl_arch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ae4df1c5943cf4748546a76d45f227f53fb8acff87095bbb53afeb5cd17aec1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
